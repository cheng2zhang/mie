\documentclass[reprint, superscriptaddress]{revtex4-1}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{mathtools}
\usepackage{upgreek}
\usepackage[table,usenames,dvipsnames]{xcolor}
%\usepackage{tikz}
%\usetikzlibrary{calc}
\usepackage{hyperref}
\usepackage{setspace}

\setcitestyle{super}

\hypersetup{
  colorlinks,
  linkcolor={blue!80!black},
  citecolor={green!20!black},
  urlcolor={blue!80!black}
}


\definecolor{DarkBlue}{RGB}{0,0,64}
\definecolor{DarkBrown}{RGB}{64,20,10}
\definecolor{DarkGreen}{RGB}{0,64,0}
\definecolor{DarkPurple}{RGB}{128,0,84}
\definecolor{LightGray}{gray}{0.85}
% annotation macros
\newcommand{\repl}[2]{{\color{gray} [#1] }{\color{blue} #2}}
\newcommand{\add}[1]{{\color{blue} #1}}
\newcommand{\del}[1]{{\color{gray} [#1]}}
\newcommand{\note}[1]{{\color{DarkGreen}\footnotesize \textsc{Note.} #1}}

\newcommand{\MI}{\mathcal I} % mutual information

\begin{document}
\title{Bias of the configuration entropy estimated from counting-based methods}

% Please edit the author list

%\author{Justin A. Drake}
%\affiliation{
%Sealy Center for Structural Biology and Molecular Biophysics,
%The University of Texas Medical Branch,
%Galveston, Texas 77555-0304, USA}
%\author{Danielle D. Seckfort}
%\affiliation{
%Quantitative and Computational Biosciences,
%Baylor College of Medicine, Houston, Texas 77030, USA}
%\author{Omneya Nassar}
%\affiliation{
%Sealy Center for Structural Biology and Molecular Biophysics,
%The University of Texas Medical Branch,
%Galveston, Texas 77555-0304, USA}
%\author{B. Montgomery Pettitt}
%\email{mpettitt@utmb.edu}
%\affiliation{
%Sealy Center for Structural Biology and Molecular Biophysics,
%The University of Texas Medical Branch,
%Galveston, Texas 77555-0304, USA}
%\affiliation{
%Quantitative and Computational Biosciences,
%Baylor College of Medicine, Houston, Texas 77030, USA}

\begin{abstract}
We show that the configuration entropy estimated from methods based on counting occurrences
carries a negative bias
that is roughly inversely proportional to the simulation length.
%
Here we propose a simple correction to the bias
and discuss its application to the mutual information expansion method.
\end{abstract}

\maketitle


\section{Introduction}

In statistical mechanics,
entropy is a key concept that
provides a natural measure of
the number of possible states a system can go through during its time evolution.
%
As a contributing part of the free energy,
change of entropy represents the potential to perform work.
%
Thus, it is possible, at least theoretically, for biological systems
to carry biological functions through entropic changes
associated with structural or conformational modifications.\cite{drake2018}


Here, we will focus on the idea of configuration entropy,
which can be defined,
for $M$ coarse-grained states in the configuration space, as
%
\begin{equation}
  S
  \equiv
  -k_B \sum_{\alpha = 1}^M p(\alpha) \, \ln p(\alpha)
  ,
  \label{eq:entropy_def}
\end{equation}
%
where $p(\alpha)$ is the probability of discrete state $\alpha = 1, \dots, M$,
and $k_B$ is the Boltzmann constant.
%
For the even distribution, $p(\alpha) \equiv 1/M$,
we can readily verify that $S \equiv k_B \, \ln M$
is proportional to the logarithm (hence a measure) of the number of states,
whereas Eq.~\eqref{eq:entropy_def}
provides a natural extension of the measure to nonuniform distributions.

We can readily apply Eq.~\eqref{eq:entropy_def} to a trajectory
obtained from a molecular dynamics (MD) or Monte Carlo (MC) simulation,
with the probabilities, $p(\alpha)$'s,
estimated from the observed occurrences of the states.
%
This straightforward application also furnishes the basis
of many advanced counting-based methods for estimating
the configuration entropy\cite{hnizdo2007, killian2007}.
%
Among them, the method of mutual information expansion (MIE)\cite{killian2007} is notable
for its potential to handle a vast but decomposable state space common in complex systems.
%
The MIE method is suitable for a many-body system
whose degrees of freedom (DOFs) are only loosely coupled.
%
Starting from the sum of marginal entropies of individual DOFs,
the method offers a cascade of levels of corrections
to systematically absorb the contributions of higher-order correlations
of increasingly larger groups, such as pairs, triplets, quartets, etc.

However, the above counting-based methods
tend to underestimate the configuration entropy
because a realistic simulation of finite length
often cannot fully sample the state space.
%
Thus, it is useful to understand the nature of this bias
and its dependence on the sample size.
%
With this understanding,
we hope to derive a suitable finite-size correction.

Here we show that the configuration entropy derived
from the counting-based methods (including the MIE method)
generally carries a negative bias
that roughly scales inversely with the trajectory length.
%
We also propose a blocking procedure
that can readily remove the bias
to the leading order.
%
%In addition, the bias of the MIE method, in particular,
%can grow dramatically with the order of tuples.
%
%In some cases, we can reduce the magnitude of the bias
%by imposing some restrictions on the higher-order tuples included.



\section{Theory and methods}

\subsection{\label{sec:fsbias}
Finite-size bias in the direct counting method}

We will first discuss the finite-size bias associated with
the direct application of Eq.~\eqref{eq:entropy_def},
which will be referred to as the direct counting method below.
%
Here, we estimate the probability, $p(\alpha)$,
by the frequency, $\hat{p}(\alpha) = \hat{n}(\alpha) / t$,
where $\hat{n}(\alpha)$ is the number of times that state $\alpha$ occurs in the trajectory of $t$ steps.
%
Then, the entropy estimate is
%
\begin{equation}
  \hat S
  =
  -k_B \sum_{\alpha = 1}^M \hat{p}(\alpha) \, \ln \hat{p}(\alpha)
  .
  \label{eq:entropy_est}
\end{equation}
%
To investigate the error of Eq.~\eqref{eq:entropy_est},
we will introduce an ensemble of replica systems,
each of which undergoes an independent simulation.
%
We can then define the error of the entropy estimate
from the difference
between the ensemble average,
$\bigl\langle \hat S \bigr\rangle$,
and the true value, $S$.
%
By the series expansion of $\hat{p}(\alpha)$ around $p(\alpha)$, we have
%
\begin{align}
  \hat S
  =
  S
  &- k_B \sum_{\alpha = 1}^M
    \bigl[\ln p(\alpha) + 1 \bigr]
    \bigl[ \hat{p}(\alpha) - p(\alpha) \bigr]
  \notag\\
  &- k_B \sum_{\alpha = 1}^M
    \frac{ \bigl[ \hat{p}(\alpha) - p(\alpha) \bigr]^2 } { 2 \, p(\alpha) }
  .
  \notag
  %\label{eq:Shat_series}
\end{align}
%
\note{The series expansion of the function, $h(x) = -x \, \ln x$,
around $x_0$,
%
\begin{align*}
  h(x)
  &\approx h(x_0) + h'(x_0) (x - x_0) + \frac{h''(x_0)}{2} (x - x_0)^2 \\
  &\approx h(x_0) - (\ln x_0 + 1) ( x - x_0) - \frac{1}{2 \, x_0} (x - x_0)^2
  .
\end{align*}
}
%
Assuming the sampling is unbiased,
we have $\langle \hat{p}(\alpha) \rangle = p(\alpha)$,
and the linear term vanishes
after ensemble averaging, and
%
\begin{align}
  \bigl\langle \hat S \bigr\rangle
  \approx
  S - k_B \sum_{\alpha = 1}^M
    \frac{ \operatorname{var} \hat{p}(\alpha) } { 2 \, p(\alpha) }
  =
  S - k_B \sum_{\alpha = 1}^M
    \frac{ \operatorname{var} \hat{n}(\alpha) } { 2 \, p(\alpha) \, t^2 }
  ,
  \label{eq:entest_2nd}
\end{align}
%
which shows that the quadratic term in the expansion
represents a nonpositive bias to the leading order.

Next we will show that the bias is inversely proportional
to the number of steps.
%
By definition, we have
%
\begin{equation*}
  \hat{n}(\alpha) = \sum_{t' = 1}^t \delta_{\alpha, \alpha(t')},
\end{equation*}
%
where $\alpha(t')$ denotes the state at step $t'$,
and $\delta_{\alpha, \gamma}$ is the Kronecker delta,
which takes the value of $1$ if $\alpha = \gamma$, or $0$ otherwise.
%
We can readily show that
\begin{align*}
  \bigl\langle \hat{n}(\alpha) \bigr\rangle
  &=
  t \, p(\alpha), \\
  %
  \operatorname{var}{\hat{n}(\alpha)}
  &=
  %\left\langle \hat{n}(\alpha)^2 \right\rangle - \langle \hat{n}(\alpha) \rangle^2
  %\\
  %&=
  t \, p(\alpha) \, \bigl( 1 - p(\alpha) \bigr) \, \bigl(2 \, \tau_\alpha + 1\bigr)
  ,
\end{align*}
%
where $\tau_\alpha$ is the integral autocorrelation time of $\delta_{\alpha, \alpha(t)}$.\footnote{
The autocorrelation function is defined as
$$
\kappa_\alpha(t) = \bigl(\delta_{\alpha, \alpha(t)} - p(\alpha)\bigr)
\bigl(\delta_{\alpha, \alpha(0)} - p(\alpha)\bigr),
$$
and the autocorrelation time is given by
$\tau_\alpha = \sum_{t = 1}^\infty \kappa_\alpha(t)/\kappa_\alpha(0)$.
}
%
Thus, we can rewrite Eq.~\eqref{eq:entest_2nd} as
%
\begin{align}
  \bigl\langle \hat S \bigr\rangle
  &\approx
  S - \frac{ C } { t }
  %\notag\\
  %&\approx
  %S - k_B
  %  \frac{ (M - 1) ( 2 \, \tau + 1) } { 2 \, t }
  ,
  \label{eq:Shat_ave}
\end{align}
%
where $C = k_B \sum_{\alpha = 1}^M
    \bigl(1 - p(\alpha)\bigr) ( 2 \, \tau_\alpha + 1)/2$.
%
For perfect sampling, $\tau_\alpha \equiv 0$, so we have $C = (M-1)/2$.
%where in the second step,
%we have assumed that all integral autocorrelation times are the same.

In summary, we have shown that the entropy estimated
from direct counting
is expected to carry a negative bias,
and, to the leading order, the bias is
inversely proportional to the trajectory length,
or sample size.



\subsection{Corrections to the linear bias}


We now propose a blocking procedure to correct the bias to the leading order.
%
We will consider the estimate from Eq.~\eqref{eq:entropy_est}
as a function of the trajectory length, $t$, and denote it as $\hat S(t)$.
%
Then, we can define an estimate, $\hat S(t_b)$,
from a trajectory segment of $t_b$ steps.
%
The ensemble average of $\hat S(t_b)$ also follows
Eq.~\eqref{eq:Shat_ave}, but with $t$ replaced by $t_b$.
%
This allows us to deduce the constants, $S$ and $C$, in Eq.~\eqref{eq:Shat_ave}
by solving the two equations for $\hat S(t)$ and $\hat S(t_b)$.
%
This process is equivalent to a two-point linear regression,
and it results in an extrapolated estimate
%
\begin{align}
    \hat S(t, t_b)
    =
    \frac{ t \, \hat S(t) - t_b \, \hat S(t_b) }
         { t - t_b }
    .
    \label{eq:S_2pt}
\end{align}

We can implement Eq.~\eqref{eq:S_2pt} in a blocking procedure.
%
We can divide the trajectory into $b$ blocks of equal length,
each of $t_b = t/b$ steps.
%
Then, we can form an estimate $S_{b, (l)}$ for each block $l = 1, \dots, b$ from Eq.~\eqref{eq:entropy_est}.
%
We then use the average of the $b$ block estimates
as the $\hat S(t_b)$ in Eq.~\eqref{eq:S_2pt}.
%
The advantage of using the block average instead of
the value from the first block is that
the former make the variance of $\hat S(t, t_b)$
roughly as low as that of $\hat S(t)$,
and thus minimizes the artifact of the extrapolation.


%\subsubsection{Higher-order improvement on the correction}
%
%Note that Eq.~\eqref{eq:Shat_ave} is only the first-order expansion
%to an infinite series,
%%
%\begin{equation}
%  \hat S = S - \frac{a_1}{t} - \frac{a_2}{t^2} - \cdots
%  .
%  \label{eq:S_series_invt}
%\end{equation}
%%
%Thus, it is possible, at least in theory,
%to further improve the estimate by a higher-order extrapolation
%against $1/t$ with more data points.
%%
%For example, if we can divide the entire trajectory to $c$ blocks
%with $c > b$ and obtain the block average of $\hat S(t_c)$
%as in the above procedure,
%%
%The three-point extrapolation estimate reads
%$$
%\hat S(t, t_b, t_c) =
%\frac{ t_b \, \hat S(t, t_b) - t_c \, \hat S(t, t_c) }
%     { t_b - t_c },
%$$
%which can eliminate not only the $1/t$ tail
%but also the higher-order $1/t^2$ tail.


%\subsubsection{Alternative extrapolation}

We can improve Eq.~\eqref{eq:S_2pt} by
fitting the function $\hat S(t)$ against a ``better'' function.
%
We note that Eq.~\eqref{eq:Shat_ave} is an asymptotic expansion
that only holds when the frequencies are close to the true probabilities,
which usually means when the effective number of samples,
$t^* = t/(\overline{2 \, \tau_\alpha + 1})$, is much greater than
the number of states $M$.
%
In initial stages, when $t^* \ll M$,
we would expect the number of states visited to be proportional to $t^*$,
and the entropy estimate, $\hat S(t)$,
to be $\ln t^* = \ln t + \mathrm{const.}$
%
A phenomenological form to accommodate both the initial and asymptotic behavior
would be
%
\begin{equation}
  \bigl\langle \hat S \bigr\rangle
  = S - \ln\left(1 + \frac{C}{t}\right)
  .
  \label{eq:expform}
\end{equation}
%
To derive a correction based on the above functional form,
we note that Eq.~\eqref{eq:expform} represents a linear relation between
$e^{-\bigl\langle \hat S \bigr\rangle}$
(instead of $\bigl\langle \hat S \bigr\rangle$ itself)
and $1/t$.
%
Thus, we can readily adapt the correction, Eq.~\eqref{eq:S_2pt},
to the exponential form as
%
\begin{equation}
  \hat S(t, t_b) = -\ln\left(
    \frac{ t \, e^{-\hat S(t)} - t_b \, e^{-\hat S(t_b)} }
         { t - t_b }
  \right)
  .
  \label{eq:S_2pt_exp}
\end{equation}
%
The linear, Eq.~\eqref{eq:S_2pt}, and exponential, Eq.~\eqref{eq:S_2pt_exp}, corrections
have a similar asymptotic behavior,
but in initial stages, the latter can sometimes
improve the accuracy of the estimate.

%As we will see, the same type of bias also exists
%in the more elaborate the MIE method.


\subsection{Mutual information expansion}


The direct counting method breaks down
for a complex system of many degrees of freedom (DOFs).
%
As the number of states increases exponentially with the number of DOFs,
the number of samples collected in each state rapidly diminishes,
which in turn deteriorates the precision of the entropy estimate.
%
For these many-body systems,
we may alternatively adopt
the mutual information expansion (MIE) method,
which is a more advanced, albeit approximate, method designed for a system
with many largely-independent DOFs.

The basic idea is that the full distribution of a multi-body system
can often be approximately derived
from the marginal and joint distributions of small groups of DOFs,
such as single DOFs, pairs, triplets, etc.
%
To the first order,
we may simply approximate the full distribution as a product
of the marginal distributions of all DOFs,
as if the DOFs were independent.
%
The correlations among pairs and triplets of DOFs
are incrementally taken into account in the second- and third-order MIE,
respectively.
%
Accordingly, the entropy of the entire system,
can be obtained from a superposition of the entropies
of the marginal and joint distributions
of small DOF groups.
%
A more detailed discussion is presented in
Supplementary Material.
%Appendix~\ref{sec:MIE_review}.

If the entropies of the small DOF groups are estimated
using the direct counting method,
the estimate of the total entropy from MIE
naturally suffers from the bias discussed below.
%
Such a bias can be readily removed
if we apply the above corrections
to each of groups.
%
This procedure is particularly simple for the linear correction.
%
Because of the linear structure of MIE,
we can apply Eq.~\eqref{eq:S_2pt}
to the final output of MIE
instead of individually to the entropy of every DOF group.



\section{Results}

\subsection{Direct-counting method}

To test the corrections we proposed for the direct-counting method,
we performed simulations of a simple random walker.
%
In each step, the random walker can assume any of the $M$ states
with equal probability without any memory.
%
The entropy of the system is simply $k_B \, \ln M$,
and we wish to see how the entropy estimated
from Eq.~\eqref{eq:entropy_est} converges to the true value,
and how the corrections accelerate the convergence.
%
Despite its simplicity, the model may bear relevance
to the sampling of metastable states of a glassy biomolecule,
which is frustrated by many moderate energetic barriers
separating metastable states.

\begin{figure}[h]\centering
  \makebox[\linewidth][c]{
    \includegraphics[angle=0, width=1.0\linewidth]{fig/walk_q1e3.pdf}
  }
  \caption{
    \label{fig:walk_q1e3}
    Estimated entropy, $\hat S$,
    versus simulation time, $t$, for a random walk over $M = 10^3$ states.
    %
    The results have been averaged over $10^5$ independent trajectories.
    The lines are a guide for the eye.
  }
\end{figure}

As shown in Fig.~\ref{fig:walk_q1e3}, for a walker of $M = 10^3$ states,
both the linear and exponential corrections,
Eqs.~\eqref{eq:S_2pt} and \eqref{eq:S_2pt_exp},
improved the convergence.
%
The errors of the entropy estimates are shown in the inset.
%
The uncorrected result
indeed carried an error roughly proportional to the inverse simulation time,
and both corrections removed the asymptotic error.
%
The exponential correction was more advantageous
at the beginning of the simulation,
whereas the linear one performed better at long times.
%
This suggests that the exponential correction
can offer greater accuracy than the linear one,
especially in initial stages.


\begin{figure}[h]\centering
  \makebox[\linewidth][c]{
    \includegraphics[angle=0, width=1.0\linewidth]{fig/walk_Svsq.pdf}
  }
  \caption{
    \label{fig:walk_Svsq}
    Estimated entropy, $\hat S$,
    against the number of states
    after $t = 10^3$ steps.
    %
    The results have been averaged over independent trajectories.
    The lines are a guide for the eye.
  }
\end{figure}

To further investigate the difference between the two corrections,
we performed simulations with different number states, $M$,
and plotted the estimated entropy against $M$
after $t = 10^3$ steps in Fig.~\ref{fig:walk_Svsq}.
%
Roughly speaking, the exponential correction performed better
when the number of states is greater than the number of steps
(i.e. in initial stages of a simulation),
and it allows a reasonable extension to
a number of states roughly $10$-times as great as the number of steps.
%
But for larger numbers of states, all methods would fail.
%
This comparison also suggests that there can be
better functional form than the linear or exponential ones used here
for correcting the entropy estimate in initial stages.



\subsection{MIE method on a linear Potts model}

We further tested the corrections for the MIE method
on a linear Potts model,
which is a linear chain of $n$ identical spins
that interact with only their nearest neighbors.
%
We wish to use the simplistic model
to capture the dihedral conformation variety
observed in disordered or flexible peptides.\cite{drake2018}
%
Each spin, representing the dihedral conformation of an amino-acid residue,
can take one of the $q = 6$ different states with equal probability.
%
The interaction between two neighboring spins
is such that there is a favorable energetic contribution of $-J$
when the two spins assume the same state, or none otherwise.
%
A more detailed description of the model is given in
Supplementary Material.
%Appendix~\ref{sec:Potts}.
%
Note that although non-neighboring spins have no direct coupling,
they are still correlated though successive or higher-order coupling,
especially at a low temperature.
%
In our test, the number of spins is $n=10$,
and the temperature is $T = 0.5$.
%
We will report our results in reduced units (with $k_B = J = 1$).


The results from the second-order MIE
(which takes into account the correlations of pairs,
but not those of triplets or larger groups)
are shown in Fig.~\ref{fig:potts_mie2nd}.
%
The error of the uncorrected estimate
scaled roughly inversely with the simulation time, $t$,
as can be seen from the inset.
%
The linear correction largely removed the inverse-time bias
and improved the entropy estimate significantly.
%
However, the remaining error dropped more slowly than $t^{-2}$.
%
This might be due to an implementation detail:
we imposed that for any pair of spins,
the entropic correction,
$\delta S_{i, j}$ (cf. Supplementary Material), %Appendix~\ref{sec:MIE_review}),
has to be nonpositive,
otherwise, zero will be used.
%
The exponential correction further improved over
the linear one in the initial stages,
while the improvement in the asymptotic regime is negligible.
%

\begin{figure}[h]\centering
  \makebox[\linewidth][c]{
    \includegraphics[angle=0, width=1.0\linewidth]{fig/potts_mie2nd.pdf}
  }
  \caption{
    \label{fig:potts_mie2nd}
    Estimated entropy from the second-order MIE method
    versus simulation time on the linear Potts model.
    %
    The reference value is the theoretical expectation
    of the second-order MIE method.
    %
    Inset: error of the estimated entropy
    versus simulation time.
    %
    The results have been averaged over $1000$ independent trajectories.
    The lines are a guide for the eye.
  }
\end{figure}


For the third-order MIE,
the linear correction again improved the entropy estimate
significantly.
However, the exponential correction was less satisfactory.
%
Unlike in the second-order case,
we can no longer ascertain the sign of
the third-order correction for each triplet,
$\delta S_{i, j, k}$ (see Supplementary Material); % Appendix~\ref{sec:MIE_review});
and thus the estimates on triplet entropies
were more susceptible to random errors,
especially for the exponential correction.
%
On balance, it appeared to be safer
to use the linear extrapolation for the MIE method,
at least for the third or higher order.

\begin{figure}[h]\centering
  \makebox[\linewidth][c]{
    \includegraphics[angle=0, width=1.0\linewidth]{fig/potts_mie3rd.pdf}
  }
  \caption{
    \label{fig:potts_mie3rd}
    Estimated entropy from the third-order MIE method
    versus simulation time on the linear Potts model.
    %
    The reference value is the theoretical expectation
    of the third-order MIE method.
    %
    Inset: error of the estimated entropy
    versus simulation time.
    %
    The results have been averaged over $1000$ independent trajectories.
    The lines are a guide for the eye.
  }
\end{figure}

\section{Conclusions}

We have shown that the configuration entropy estimated from counting-based methods
generally carries a negative bias whose magnitude
is roughly inversely proportional to the simulation length.
%
Based on this observation, we have proposed a simple blocking procedure
to remove the bias to the leading order.
%
We hope the findings presented here to be useful
for the biophysical community
in investigating the thermodynamic properties of
biologically important systems.


\bibliography{simul}
\end{document}
